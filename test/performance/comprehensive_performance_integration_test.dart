import 'dart:async';
import 'dart:math' as math;

import 'package:flutter_test/flutter_test.dart';
import 'package:mockito/mockito.dart';
import 'package:task_tracker_app/domain/entities/task_model.dart';
import 'package:task_tracker_app/domain/entities/project.dart' as entities;
import 'package:task_tracker_app/domain/models/enums.dart';
import 'package:task_tracker_app/services/database/database.dart' as db;
import 'package:task_tracker_app/services/analytics/analytics_service.dart';

import '../mocks/test_mocks.mocks.dart';

void main() {
  group('Comprehensive Performance Integration Tests - Enterprise Scale', () {
    late MockTaskRepository mockTaskRepository;
    late MockProjectRepository mockProjectRepository;
    late MockAnalyticsService mockAnalyticsService;
    late MockAppDatabase mockDatabase;
    late PerformanceBenchmarker benchmarker;
    late DatabasePerformanceTracker dbTracker;

    setUp(() {
      mockTaskRepository = MockTaskRepository();
      mockProjectRepository = MockProjectRepository();
      mockAnalyticsService = MockAnalyticsService();
      mockDatabase = MockAppDatabase();
      benchmarker = PerformanceBenchmarker();
      dbTracker = DatabasePerformanceTracker();
    });

    group('Database Query Performance at Scale', () {
      test('Complex task queries with 50k+ tasks perform within benchmarks', () async {
        final tasks = _generateMassiveTaskDataset(50000);
        final projects = _generateProjectDataset(500);
        
        when(mockTaskRepository.getAllTasks()).thenAnswer((_) async => tasks);
        when(mockProjectRepository.getAllProjects()).thenAnswer((_) async => projects);
        
        // Test complex query operations
        final queryTests = [
          () => mockTaskRepository.getTasksByStatus(TaskStatus.inProgress),
          () => mockTaskRepository.getTasksByPriority(TaskPriority.high),
          () => mockTaskRepository.getTasksByDateRange(
            DateTime.now().subtract(const Duration(days: 30)), 
            DateTime.now()
          ),
          () => mockTaskRepository.searchTasks('critical'),
          () => mockTaskRepository.getTasksByProject('project-1'),
          () => mockTaskRepository.getOverdueTasks(),
        ];
        
        for (int i = 0; i < queryTests.length; i++) {
          final stopwatch = Stopwatch()..start();
          
          // Mock the query with realistic filtering simulation
          when(queryTests[i]()).thenAnswer((_) async {
            await Future.delayed(const Duration(milliseconds: 50)); // Simulate DB time
            return tasks.take(1000).toList(); // Return subset
          });
          
          await queryTests[i]();
          
          stopwatch.stop();
          
          benchmarker.recordMetric('db_complex_query_$i', stopwatch.elapsedMilliseconds);
          
          // Database queries should complete within 500ms for large datasets
          expect(stopwatch.elapsedMilliseconds, lessThan(500),
                 reason: 'Complex database query $i should complete within 500ms');
        }
        
        print('All complex database queries completed within performance benchmarks');
      });

      test('Batch operations performance with massive datasets', () async {
        final batchSizes = [100, 500, 1000, 2500, 5000];
        
        for (final batchSize in batchSizes) {
          final tasks = _generateMassiveTaskDataset(batchSize);
          
          // Mock batch insert
          when(mockTaskRepository.insertTasks(any)).thenAnswer((_) async {
            await Future.delayed(Duration(milliseconds: batchSize ~/ 10));
            return tasks;
          });
          
          final stopwatch = Stopwatch()..start();
          
          await mockTaskRepository.insertTasks(tasks);
          
          stopwatch.stop();
          
          benchmarker.recordMetric('db_batch_insert_$batchSize', stopwatch.elapsedMilliseconds);
          
          // Batch operations should scale linearly
          final expectedMaxTime = (batchSize / 10).round(); // 10ms per 100 items
          expect(stopwatch.elapsedMilliseconds, lessThan(expectedMaxTime + 1000),
                 reason: 'Batch insert of $batchSize tasks should complete within ${expectedMaxTime + 1000}ms');
          
          print('Batch insert ($batchSize tasks): ${stopwatch.elapsedMilliseconds}ms');
        }
      });

      test('Database transaction performance under load', () async {
        const transactionOperations = 1000;
        
        // Mock transaction operations
        when(mockDatabase.transaction(any)).thenAnswer((_) async {
          await Future.delayed(const Duration(microseconds: 500)); // Realistic transaction time
          return null;
        });
        
        final stopwatch = Stopwatch()..start();
        
        // Simulate complex transaction with multiple operations
        final futures = List.generate(transactionOperations, (i) async {
          return await mockDatabase.transaction(() async {
            // Simulate multiple operations in transaction
            await Future.delayed(const Duration(microseconds: 100));
            return i;
          });
        });
        
        await Future.wait(futures);
        
        stopwatch.stop();
        
        benchmarker.recordMetric('db_transaction_load_test', stopwatch.elapsedMilliseconds);
        
        final avgTransactionTime = stopwatch.elapsedMilliseconds / transactionOperations;
        
        // Transactions should be processed efficiently
        expect(avgTransactionTime, lessThan(5.0),
               reason: 'Average transaction time should be under 5ms');
        expect(stopwatch.elapsedMilliseconds, lessThan(10000),
               reason: '1000 transactions should complete within 10 seconds');
        
        print('Database transaction performance: ${avgTransactionTime.toStringAsFixed(2)}ms avg');
      });

      test('Database index performance validation', () async {
        final indexedQueries = [
          'task_status_index',
          'task_priority_index', 
          'task_due_date_index',
          'task_project_id_index',
          'task_created_at_index',
          'project_name_index',
        ];
        
        for (final indexName in indexedQueries) {
          // Mock indexed query performance
          final stopwatch = Stopwatch()..start();
          
          // Simulate indexed query with realistic performance
          await Future.delayed(Duration(milliseconds: math.Random().nextInt(20) + 5));
          
          stopwatch.stop();
          
          benchmarker.recordMetric('db_indexed_query_$indexName', stopwatch.elapsedMilliseconds);
          
          // Indexed queries should be very fast
          expect(stopwatch.elapsedMilliseconds, lessThan(50),
                 reason: 'Indexed query $indexName should complete within 50ms');
        }
        
        print('All database index queries performed within benchmarks');
      });
    });

    group('Full-Stack Performance Integration', () {
      test('End-to-end Kanban board data flow performance', () async {
        final tasks = _generateMassiveTaskDataset(10000);
        final projects = _generateProjectDataset(100);
        
        when(mockTaskRepository.watchAllTasks()).thenAnswer(
          (_) => Stream.value(tasks),
        );
        when(mockProjectRepository.watchAllProjects()).thenAnswer(
          (_) => Stream.value(projects),
        );
        
        final e2eStopwatch = Stopwatch()..start();
        
        // Simulate full data flow: DB -> Repository -> Provider -> UI
        final dataStream = mockTaskRepository.watchAllTasks();
        final projectStream = mockProjectRepository.watchAllProjects();
        
        // Wait for data processing
        await Future.wait([
          dataStream.first,
          projectStream.first,
        ]);
        
        // Simulate UI processing time
        await Future.delayed(const Duration(milliseconds: 100));
        
        e2eStopwatch.stop();
        
        benchmarker.recordMetric('e2e_kanban_data_flow_10k', e2eStopwatch.elapsedMilliseconds);
        
        // End-to-end should complete within reasonable time for good UX
        expect(e2eStopwatch.elapsedMilliseconds, lessThan(5000),
               reason: 'End-to-end data flow (10k tasks) should complete within 5 seconds');
        
        print('E2E Kanban data flow (10k tasks): ${e2eStopwatch.elapsedMilliseconds}ms');
      });

      test('Analytics computation performance with large datasets', () async {
        final tasks = _generateMassiveTaskDataset(25000);
        final projects = _generateProjectDataset(200);
        
        final analyticsService = AnalyticsService();
        
        // Mock analytics computations
        when(mockAnalyticsService.calculateProductivityMetrics(any)).thenAnswer((_) async {
          await Future.delayed(const Duration(milliseconds: 2000));
          return ProductivityMetrics.empty();
        });
        
        when(mockAnalyticsService.calculateTaskStatistics(any)).thenAnswer((_) async {
          await Future.delayed(const Duration(milliseconds: 1500));
          return TaskStatistics.empty();
        });
        
        final stopwatch = Stopwatch()..start();
        
        // Run analytics computations
        final futures = [
          mockAnalyticsService.calculateProductivityMetrics(tasks),
          mockAnalyticsService.calculateTaskStatistics(tasks),
        ];
        
        await Future.wait(futures);
        
        stopwatch.stop();
        
        benchmarker.recordMetric('analytics_computation_25k', stopwatch.elapsedMilliseconds);
        
        // Analytics should complete within acceptable time
        expect(stopwatch.elapsedMilliseconds, lessThan(8000),
               reason: 'Analytics computation (25k tasks) should complete within 8 seconds');
        
        print('Analytics computation (25k tasks): ${stopwatch.elapsedMilliseconds}ms');
      });

      test('Real-time updates performance under high load', () async {
        final updateController = StreamController<List<TaskModel>>.broadcast();
        final initialTasks = _generateMassiveTaskDataset(5000);
        
        when(mockTaskRepository.watchAllTasks()).thenAnswer(
          (_) => updateController.stream,
        );
        
        // Start listening to updates
        final updateStopwatch = Stopwatch()..start();
        int updateCount = 0;
        
        final subscription = mockTaskRepository.watchAllTasks().listen((tasks) {
          updateCount++;
        });
        
        // Send rapid updates
        for (int i = 0; i < 100; i++) {
          final updatedTasks = List<TaskModel>.from(initialTasks);
          // Simulate task updates
          if (i < initialTasks.length) {
            updatedTasks[i] = initialTasks[i].copyWith(
              updatedAt: DateTime.now(),
            );
          }
          
          updateController.add(updatedTasks);
          await Future.delayed(const Duration(milliseconds: 10));
        }
        
        // Wait for all updates to be processed
        await Future.delayed(const Duration(milliseconds: 500));
        updateStopwatch.stop();
        
        await subscription.cancel();
        await updateController.close();
        
        benchmarker.recordMetric('realtime_updates_high_load', updateStopwatch.elapsedMilliseconds);
        
        final avgUpdateTime = updateStopwatch.elapsedMilliseconds / updateCount;
        
        // Real-time updates should be processed efficiently
        expect(avgUpdateTime, lessThan(50.0),
               reason: 'Average real-time update processing should be under 50ms');
        expect(updateCount, equals(100),
               reason: 'All updates should be processed');
        
        print('Real-time updates performance: ${avgUpdateTime.toStringAsFixed(2)}ms avg ($updateCount updates)');
      });
    });

    group('Memory and Resource Management', () {
      test('Memory usage under extreme load conditions', () async {
        final memoryTracker = MemoryUsageTracker();
        
        // Baseline memory
        final baselineMemory = await memoryTracker.getCurrentUsage();
        
        // Create extreme dataset
        final extremeTasks = _generateMassiveTaskDataset(100000); // 100k tasks
        final extremeProjects = _generateProjectDataset(1000);   // 1k projects
        
        final afterDataCreation = await memoryTracker.getCurrentUsage();
        
        // Simulate processing operations
        when(mockTaskRepository.getAllTasks()).thenAnswer((_) async => extremeTasks);
        when(mockProjectRepository.getAllProjects()).thenAnswer((_) async => extremeProjects);
        
        await mockTaskRepository.getAllTasks();
        await mockProjectRepository.getAllProjects();
        
        final afterProcessing = await memoryTracker.getCurrentUsage();
        
        // Cleanup
        extremeTasks.clear();
        extremeProjects.clear();
        
        await memoryTracker.forceGarbageCollection();
        final afterCleanup = await memoryTracker.getCurrentUsage();
        
        final dataMemoryUsage = afterDataCreation - baselineMemory;
        final processingMemoryUsage = afterProcessing - afterDataCreation;
        final memoryRecovered = afterProcessing - afterCleanup;
        
        benchmarker.recordMetric('memory_extreme_load_data', dataMemoryUsage.round());
        benchmarker.recordMetric('memory_extreme_load_processing', processingMemoryUsage.round());
        benchmarker.recordMetric('memory_extreme_load_recovered', memoryRecovered.round());
        
        // Memory should be managed efficiently even under extreme load
        expect(dataMemoryUsage, lessThan(1000.0),
               reason: 'Data creation (100k tasks) should use less than 1GB memory');
        expect(processingMemoryUsage, lessThan(500.0),
               reason: 'Processing should not cause excessive memory growth');
        expect(memoryRecovered, greaterThan(dataMemoryUsage * 0.6),
               reason: 'At least 60% of memory should be recoverable');
        
        print('Extreme load memory usage:');
        print('  Data: ${dataMemoryUsage.toStringAsFixed(1)}MB');
        print('  Processing: ${processingMemoryUsage.toStringAsFixed(1)}MB');
        print('  Recovered: ${memoryRecovered.toStringAsFixed(1)}MB');
      });

      test('Resource cleanup and leak prevention', () async {
        final resourceTracker = ResourceTracker();
        
        // Run multiple cycles to detect leaks
        for (int cycle = 0; cycle < 20; cycle++) {
          // Create resources
          final tasks = _generateMassiveTaskDataset(2000);
          final streamController = StreamController<List<TaskModel>>.broadcast();
          
          when(mockTaskRepository.watchAllTasks()).thenAnswer(
            (_) => streamController.stream,
          );
          
          // Use resources
          final subscription = mockTaskRepository.watchAllTasks().listen((tasks) {
            // Process tasks
          });
          
          streamController.add(tasks);
          await Future.delayed(const Duration(milliseconds: 50));
          
          // Cleanup resources
          await subscription.cancel();
          await streamController.close();
          tasks.clear();
          
          await resourceTracker.recordResourceUsage(cycle);
        }
        
        final resourceTrend = resourceTracker.analyzeResourceTrend();
        
        benchmarker.recordMetric('resource_leak_trend', (resourceTrend * 1000).round());
        
        // Resource usage should not show continuous upward trend
        expect(resourceTrend, lessThan(1.0),
               reason: 'Resource usage should not show significant upward trend');
        
        print('Resource leak analysis: ${resourceTrend > 0 ? '+' : ''}${resourceTrend.toStringAsFixed(3)} MB/cycle');
      });
    });

    group('Performance Benchmarking and Validation', () {
      test('All performance benchmarks meet enterprise requirements', () async {
        final summary = benchmarker.generateSummary();
        
        final enterpriseRequirements = {
          // Database Performance
          'db_complex_query_0': 500,    // Complex queries < 500ms
          'db_batch_insert_1000': 2000, // 1k batch insert < 2s
          'db_transaction_load_test': 10000, // 1k transactions < 10s
          
          // Integration Performance  
          'e2e_kanban_data_flow_10k': 5000,   // E2E flow < 5s
          'analytics_computation_25k': 8000,   // Analytics < 8s
          'realtime_updates_high_load': 5000,  // Updates < 5s
          
          // Memory Requirements
          'memory_extreme_load_data': 1000,      // Data < 1GB
          'memory_extreme_load_processing': 500, // Processing < 500MB
        };
        
        final failures = <String>[];
        final successes = <String>[];
        
        enterpriseRequirements.forEach((metric, maxValue) {
          if (summary.containsKey(metric)) {
            final avg = summary[metric]!['avg']!;
            if (avg > maxValue) {
              failures.add('âŒ $metric: ${avg}ms > ${maxValue}ms');
            } else {
              successes.add('âœ… $metric: ${avg}ms â‰¤ ${maxValue}ms');
            }
          }
        });
        
        print('\nEnterprise Performance Requirements:');
        successes.forEach(print);
        
        if (failures.isNotEmpty) {
          print('\nFAILED Requirements:');
          failures.forEach(print);
        } else {
          print('\nðŸŽ‰ All enterprise performance requirements MET!');
        }
        
        expect(failures, isEmpty, reason: 'Enterprise performance requirements not met');
      });

      test('Performance regression detection and alerting', () async {
        final regressionDetector = PerformanceRegressionDetector();
        
        // Load historical baselines (in real app, these would be from CI/storage)
        final historicalBaselines = {
          'db_complex_query_0': [245, 268, 251, 239, 277], // Last 5 runs
          'e2e_kanban_data_flow_10k': [3200, 3150, 3380, 3210, 3190],
          'analytics_computation_25k': [5800, 5950, 5720, 5890, 5810],
        };
        
        // Current run results (simulated)
        final currentResults = {
          'db_complex_query_0': 290,        // Slight regression
          'e2e_kanban_data_flow_10k': 3250, // Within normal range
          'analytics_computation_25k': 6200, // Noticeable regression
        };
        
        final regressions = <String, RegressionAnalysis>{};
        
        historicalBaselines.forEach((metric, historical) {
          if (currentResults.containsKey(metric)) {
            final analysis = regressionDetector.analyzeRegression(
              historical, 
              currentResults[metric]!
            );
            
            if (analysis.hasRegression) {
              regressions[metric] = analysis;
            }
          }
        });
        
        if (regressions.isNotEmpty) {
          print('\nâš ï¸ Performance Regressions Detected:');
          regressions.forEach((metric, analysis) {
            print('  $metric: ${analysis.regressionPercent.toStringAsFixed(1)}% slower (${analysis.severity})');
          });
        } else {
          print('\nâœ… No performance regressions detected');
        }
        
        // Only fail on critical regressions for CI
        final criticalRegressions = regressions.values
            .where((analysis) => analysis.severity == 'CRITICAL')
            .toList();
        
        expect(criticalRegressions, isEmpty,
               reason: 'Critical performance regressions detected');
      });
    });

    tearDown(() {
      final summary = benchmarker.generateSummary();
      print('\n=== Comprehensive Performance Integration Summary ===');
      
      final categories = {
        'ðŸ—„ï¸  Database Performance': summary.entries.where((e) => e.key.startsWith('db_')),
        'ðŸ”„ Integration Performance': summary.entries.where((e) => e.key.startsWith('e2e_') || e.key.contains('analytics')),
        'ðŸ§  Memory Performance': summary.entries.where((e) => e.key.startsWith('memory_')),
        'âš¡ Real-time Performance': summary.entries.where((e) => e.key.contains('realtime') || e.key.contains('updates')),
      };
      
      categories.forEach((category, metrics) {
        if (metrics.isNotEmpty) {
          print('\n$category:');
          for (final metric in metrics) {
            final stats = metric.value;
            final unit = metric.key.contains('memory') ? 'MB' : 'ms';
            print('  ${metric.key}: ${stats['avg']}$unit avg (${stats['min']}-${stats['max']}$unit)');
          }
        }
      });
      
      print('\n======================================================\n');
    });
  });
}

/// Generates massive task dataset for performance testing
List<TaskModel> _generateMassiveTaskDataset(int count) {
  final tasks = <TaskModel>[];
  final now = DateTime.now();
  final random = math.Random(12345); // Fixed seed for reproducible tests
  
  for (int i = 0; i < count; i++) {
    final task = TaskModel.create(
      title: 'Performance Task #${i + 1}',
      description: 'Generated task for performance testing with realistic data content',
      priority: TaskPriority.values[random.nextInt(TaskPriority.values.length)],
      tags: _generateRandomTags(random),
      projectId: random.nextBool() ? 'project-${random.nextInt(100)}' : null,
      dueDate: random.nextBool() 
        ? now.add(Duration(days: random.nextInt(90) - 30))
        : null,
      estimatedDuration: Duration(minutes: 15 + random.nextInt(480)),
      createdAt: now.subtract(Duration(days: random.nextInt(180))),
    ).copyWith(
      completedAt: random.nextDouble() < 0.7 
        ? now.subtract(Duration(days: random.nextInt(30)))
        : null,
    );
    
    tasks.add(task);
  }
  
  return tasks;
}

/// Generates project dataset for testing
List<entities.Project> _generateProjectDataset(int count) {
  final projects = <entities.Project>[];
  final random = math.Random(12345);
  
  for (int i = 0; i < count; i++) {
    final project = entities.Project.create(
      name: 'Performance Project ${i + 1}',
      description: 'Generated project for performance testing',
      deadline: random.nextBool() 
        ? DateTime.now().add(Duration(days: 30 + random.nextInt(90)))
        : null,
    );
    
    projects.add(project);
  }
  
  return projects;
}

/// Generates random tags for tasks
List<String> _generateRandomTags(math.Random random) {
  final allTags = [
    'performance', 'testing', 'critical', 'feature', 'bug', 'ui', 'backend',
    'api', 'database', 'security', 'optimization', 'mobile', 'web', 'integration'
  ];
  
  final tagCount = 1 + random.nextInt(3);
  final selectedTags = <String>[];
  
  for (int i = 0; i < tagCount; i++) {
    final tag = allTags[random.nextInt(allTags.length)];
    if (!selectedTags.contains(tag)) {
      selectedTags.add(tag);
    }
  }
  
  return selectedTags;
}

/// Performance benchmarker utility
class PerformanceBenchmarker {
  final Map<String, List<int>> _metrics = {};
  
  void recordMetric(String name, int valueMs) {
    _metrics.putIfAbsent(name, () => []).add(valueMs);
  }
  
  Map<String, Map<String, int>> generateSummary() {
    final summary = <String, Map<String, int>>{};
    
    _metrics.forEach((name, values) {
      final avg = values.reduce((a, b) => a + b) / values.length;
      final min = values.reduce(math.min);
      final max = values.reduce(math.max);
      
      summary[name] = {
        'avg': avg.round(),
        'min': min,
        'max': max,
        'count': values.length,
      };
    });
    
    return summary;
  }
}

/// Database performance tracking utility
class DatabasePerformanceTracker {
  final List<QueryPerformance> _queries = [];
  
  void recordQuery(String query, int durationMs, int rowsAffected) {
    _queries.add(QueryPerformance(query, durationMs, rowsAffected));
  }
  
  Map<String, double> getQueryStatistics() {
    if (_queries.isEmpty) return {};
    
    final avgDuration = _queries.map((q) => q.durationMs).reduce((a, b) => a + b) / _queries.length;
    final maxDuration = _queries.map((q) => q.durationMs).reduce(math.max);
    final totalRows = _queries.map((q) => q.rowsAffected).reduce((a, b) => a + b);
    
    return {
      'avgDuration': avgDuration,
      'maxDuration': maxDuration.toDouble(),
      'totalRows': totalRows.toDouble(),
      'queryCount': _queries.length.toDouble(),
    };
  }
}

/// Memory usage tracker utility
class MemoryUsageTracker {
  Future<double> getCurrentUsage() async {
    await Future.delayed(const Duration(milliseconds: 10));
    return 100.0 + (DateTime.now().millisecondsSinceEpoch % 1000) / 20.0;
  }
  
  Future<void> forceGarbageCollection() async {
    await Future.delayed(const Duration(milliseconds: 100));
  }
}

/// Resource usage tracker
class ResourceTracker {
  final List<double> _resourceReadings = [];
  
  Future<void> recordResourceUsage(int cycle) async {
    // Simulate resource usage measurement
    final usage = 50.0 + (cycle * 0.1) + (math.Random().nextDouble() * 5);
    _resourceReadings.add(usage);
    await Future.delayed(const Duration(milliseconds: 10));
  }
  
  double analyzeResourceTrend() {
    if (_resourceReadings.length < 2) return 0.0;
    
    // Simple linear trend calculation
    final first = _resourceReadings.take(5).fold(0.0, (a, b) => a + b) / 5;
    final last = _resourceReadings.skip(_resourceReadings.length - 5).fold(0.0, (a, b) => a + b) / 5;
    
    return (last - first) / _resourceReadings.length;
  }
}

/// Performance regression detector
class PerformanceRegressionDetector {
  RegressionAnalysis analyzeRegression(List<int> historical, int current) {
    final baseline = historical.fold(0, (a, b) => a + b) / historical.length;
    final regressionPercent = ((current - baseline) / baseline) * 100;
    
    String severity;
    bool hasRegression = false;
    
    if (regressionPercent > 50) {
      severity = 'CRITICAL';
      hasRegression = true;
    } else if (regressionPercent > 20) {
      severity = 'WARNING';
      hasRegression = true;
    } else if (regressionPercent > 10) {
      severity = 'MINOR';
      hasRegression = true;
    } else {
      severity = 'NONE';
      hasRegression = false;
    }
    
    return RegressionAnalysis(
      regressionPercent: regressionPercent,
      severity: severity,
      hasRegression: hasRegression,
      baseline: baseline,
      current: current.toDouble(),
    );
  }
}

/// Data classes
class QueryPerformance {
  final String query;
  final int durationMs;
  final int rowsAffected;
  
  const QueryPerformance(this.query, this.durationMs, this.rowsAffected);
}

class RegressionAnalysis {
  final double regressionPercent;
  final String severity;
  final bool hasRegression;
  final double baseline;
  final double current;
  
  const RegressionAnalysis({
    required this.regressionPercent,
    required this.severity,
    required this.hasRegression,
    required this.baseline,
    required this.current,
  });
}

// Mock data classes for analytics
class ProductivityMetrics {
  static ProductivityMetrics empty() => const ProductivityMetrics();
  
  const ProductivityMetrics();
}

class TaskStatistics {
  static TaskStatistics empty() => const TaskStatistics();
  
  const TaskStatistics();
}